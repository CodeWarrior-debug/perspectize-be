---
phase: 07.4-performance-monitoring
plan: 01
type: execute
wave: 1
depends_on: ["07.3"]
files_modified:
  - backend/pkg/middleware/timing.go
  - backend/pkg/middleware/timing_test.go
  - backend/pkg/database/plugins.go
  - backend/pkg/database/plugins_test.go
  - backend/pkg/database/stats.go
  - backend/pkg/database/stats_test.go
  - backend/pkg/graphql/timing.go
  - backend/pkg/graphql/timing_test.go
  - backend/cmd/server/main.go
  - backend/internal/core/services/content_service_bench_test.go
  - backend/internal/core/services/perspective_service_bench_test.go
  - frontend/src/lib/vitals.ts
  - frontend/src/routes/+layout.svelte
autonomous: true

must_haves:
  truths:
    - "Every HTTP request logs method, path, status code, and duration_ms via slog structured fields"
    - "RequestID from chi middleware is propagated into slog log lines"
    - "GORM callbacks log slow queries (>100ms) with the SQL statement and duration"
    - "A /debug/db-stats endpoint returns sql.DBStats as JSON (open connections, in-use, idle, wait count)"
    - "gqlgen AroundOperations extension logs GraphQL operation name and duration_ms"
    - "Go benchmark tests exist for ContentService and PerspectiveService with mocked repositories"
    - "Frontend captures Core Web Vitals (LCP, CLS, INP) and logs them to console in dev"
    - "All existing backend tests pass"
    - "All existing frontend tests pass"
  artifacts:
    - path: "backend/pkg/middleware/timing.go"
      provides: "HTTP request timing middleware with slog structured logging"
      exports: ["RequestTimer"]
    - path: "backend/pkg/database/plugins.go"
      provides: "GORM slow query callback plugin"
      exports: ["RegisterSlowQueryLogger"]
    - path: "backend/pkg/database/stats.go"
      provides: "sql.DBStats JSON endpoint handler"
      exports: ["StatsHandler"]
    - path: "backend/pkg/graphql/timing.go"
      provides: "gqlgen operation timing extension"
      exports: ["OperationTimer"]
    - path: "frontend/src/lib/vitals.ts"
      provides: "Web Vitals reporting module"
      exports: ["reportWebVitals"]
  key_links:
    - from: "backend/cmd/server/main.go"
      to: "backend/pkg/middleware/timing.go"
      via: "r.Use(middleware.RequestTimer)"
      pattern: "RequestTimer"
    - from: "backend/cmd/server/main.go"
      to: "backend/pkg/database/plugins.go"
      via: "database.RegisterSlowQueryLogger(db)"
      pattern: "RegisterSlowQueryLogger"
    - from: "backend/cmd/server/main.go"
      to: "backend/pkg/graphql/timing.go"
      via: "srv.Use(graphqltiming.OperationTimer())"
      pattern: "OperationTimer"
---

<objective>
Add performance monitoring baselines across the full stack: HTTP request timing middleware, GORM slow query logging, DB connection pool stats endpoint, GraphQL resolver timing, Go service benchmarks, and frontend Web Vitals capture.

Purpose: Establish measurable performance baselines before the application grows in complexity. All instrumentation uses slog (no new infrastructure dependencies) except the frontend web-vitals npm package.

Output: 6 observability capabilities wired into the existing backend and frontend, plus benchmark tests for reproducible service-layer performance numbers.
</objective>

<execution_context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
</execution_context>

<context>
@backend/CLAUDE.md
@backend/cmd/server/main.go
@backend/pkg/database/postgres.go
@backend/internal/adapters/graphql/resolvers/resolver.go
@backend/internal/adapters/graphql/resolvers/schema.resolvers.go
@backend/internal/core/services/content_service.go
@backend/internal/core/services/perspective_service.go
@frontend/src/routes/+layout.svelte
@frontend/src/app.html
</context>

<tasks>

<task type="auto">
  <name>Task 1: Request timing middleware with slog structured logging</name>
  <files>
    backend/pkg/middleware/timing.go
    backend/pkg/middleware/timing_test.go
  </files>
  <action>
    1. Create `backend/pkg/middleware/timing.go`:

    ```go
    package middleware

    import (
        "log/slog"
        "net/http"
        "time"

        chimw "github.com/go-chi/chi/v5/middleware"
    )

    // RequestTimer logs structured timing data for every HTTP request.
    // Must be placed AFTER chi's middleware.RequestID in the middleware chain.
    func RequestTimer(next http.Handler) http.Handler {
        return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
            start := time.Now()
            ww := chimw.NewWrapResponseWriter(w, r.ProtoMajor)

            next.ServeHTTP(ww, r)

            slog.InfoContext(r.Context(), "request",
                "method", r.Method,
                "path", r.URL.Path,
                "status", ww.Status(),
                "duration_ms", time.Since(start).Milliseconds(),
                "bytes", ww.BytesWritten(),
                "request_id", chimw.GetReqID(r.Context()),
            )
        })
    }
    ```

    2. Create `backend/pkg/middleware/timing_test.go`:

    ```go
    package middleware_test

    import (
        "net/http"
        "net/http/httptest"
        "testing"

        chimw "github.com/go-chi/chi/v5/middleware"
        "github.com/CodeWarrior-debug/perspectize/backend/pkg/middleware"
        "github.com/stretchr/testify/assert"
    )

    func TestRequestTimer(t *testing.T) {
        handler := chimw.RequestID(middleware.RequestTimer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
            w.WriteHeader(http.StatusOK)
            w.Write([]byte("ok"))
        })))

        req := httptest.NewRequest(http.MethodGet, "/test", nil)
        rec := httptest.NewRecorder()

        handler.ServeHTTP(rec, req)

        assert.Equal(t, http.StatusOK, rec.Code)
        assert.Equal(t, "ok", rec.Body.String())
    }

    func TestRequestTimer_RecordsStatus(t *testing.T) {
        handler := middleware.RequestTimer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
            w.WriteHeader(http.StatusNotFound)
        }))

        req := httptest.NewRequest(http.MethodGet, "/missing", nil)
        rec := httptest.NewRecorder()

        handler.ServeHTTP(rec, req)

        assert.Equal(t, http.StatusNotFound, rec.Code)
    }
    ```

    3. Run `cd backend && go test ./pkg/middleware/...` — must pass.
  </action>
  <verify>
    `cd backend && go test ./pkg/middleware/... -v` passes. `grep "RequestTimer" backend/pkg/middleware/timing.go` returns match.
  </verify>
  <done>
    Request timing middleware created with slog structured fields (method, path, status, duration_ms, bytes, request_id). Tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: GORM slow query logger and DB stats endpoint</name>
  <files>
    backend/pkg/database/plugins.go
    backend/pkg/database/plugins_test.go
    backend/pkg/database/stats.go
    backend/pkg/database/stats_test.go
  </files>
  <action>
    1. Create `backend/pkg/database/plugins.go`:

    ```go
    package database

    import (
        "log/slog"
        "time"

        "gorm.io/gorm"
    )

    const slowQueryThreshold = 100 * time.Millisecond

    // RegisterSlowQueryLogger adds GORM callbacks that log queries exceeding the threshold.
    func RegisterSlowQueryLogger(db *gorm.DB) {
        callback := func(operation string) func(db *gorm.DB) {
            return func(db *gorm.DB) {
                if db.Statement == nil {
                    return
                }
                elapsed := db.Statement.Context.Value(queryStartKey{})
                if elapsed == nil {
                    return
                }
                duration := time.Since(elapsed.(time.Time))
                if duration >= slowQueryThreshold {
                    slog.WarnContext(db.Statement.Context, "slow query",
                        "operation", operation,
                        "duration_ms", duration.Milliseconds(),
                        "sql", db.Statement.SQL.String(),
                        "rows", db.Statement.RowsAffected,
                    )
                }
            }
        }

        startTimer := func(db *gorm.DB) {
            db.Statement.Context = setQueryStart(db.Statement.Context, time.Now())
        }

        _ = db.Callback().Query().Before("gorm:query").Register("perf:query_start", startTimer)
        _ = db.Callback().Query().After("gorm:query").Register("perf:query_slow", callback("query"))
        _ = db.Callback().Create().Before("gorm:create").Register("perf:create_start", startTimer)
        _ = db.Callback().Create().After("gorm:create").Register("perf:create_slow", callback("create"))
        _ = db.Callback().Update().Before("gorm:update").Register("perf:update_start", startTimer)
        _ = db.Callback().Update().After("gorm:update").Register("perf:update_slow", callback("update"))
        _ = db.Callback().Delete().Before("gorm:delete").Register("perf:delete_start", startTimer)
        _ = db.Callback().Delete().After("gorm:delete").Register("perf:delete_slow", callback("delete"))
    }

    type queryStartKey struct{}

    func setQueryStart(ctx context.Context, t time.Time) context.Context {
        return context.WithValue(ctx, queryStartKey{}, t)
    }
    ```

    Note: Add `"context"` to the import block.

    2. Create `backend/pkg/database/plugins_test.go`:

    ```go
    package database_test

    import (
        "testing"

        "github.com/stretchr/testify/assert"
        "gorm.io/driver/sqlite"
        "gorm.io/gorm"
        "github.com/CodeWarrior-debug/perspectize/backend/pkg/database"
    )

    func TestRegisterSlowQueryLogger_DoesNotPanic(t *testing.T) {
        // Verify the callback registration doesn't panic on a real GORM instance.
        // We use SQLite in-memory since we just need a valid *gorm.DB.
        db, err := gorm.Open(sqlite.Open(":memory:"), &gorm.Config{})
        if err != nil {
            t.Skip("SQLite not available, skipping GORM callback test")
        }

        assert.NotPanics(t, func() {
            database.RegisterSlowQueryLogger(db)
        })
    }
    ```

    WAIT — SQLite adds a dependency. Instead, test that the function accepts a *gorm.DB by using the existing postgres connection pattern or just test the function signature. Actually, we can use the existing ConnectGORM with a test DSN, but that requires a running DB. Let's keep the test simple and focused: verify the function doesn't panic when called with a properly initialized GORM instance. We'll skip the test if no DB is available.

    Revised `backend/pkg/database/plugins_test.go`:

    ```go
    package database_test

    import (
        "os"
        "testing"

        "github.com/CodeWarrior-debug/perspectize/backend/pkg/database"
        "github.com/stretchr/testify/assert"
    )

    func TestRegisterSlowQueryLogger(t *testing.T) {
        dsn := os.Getenv("DATABASE_URL")
        if dsn == "" {
            t.Skip("DATABASE_URL not set, skipping slow query logger test")
        }

        db, err := database.ConnectGORM(dsn, database.DefaultPoolConfig())
        if err != nil {
            t.Skip("cannot connect to database, skipping")
        }
        sqlDB, _ := db.DB()
        defer sqlDB.Close()

        assert.NotPanics(t, func() {
            database.RegisterSlowQueryLogger(db)
        })
    }
    ```

    3. Create `backend/pkg/database/stats.go`:

    ```go
    package database

    import (
        "database/sql"
        "encoding/json"
        "net/http"
    )

    // StatsHandler returns an http.HandlerFunc that reports sql.DBStats as JSON.
    func StatsHandler(sqlDB *sql.DB) http.HandlerFunc {
        return func(w http.ResponseWriter, r *http.Request) {
            stats := sqlDB.Stats()
            w.Header().Set("Content-Type", "application/json")
            json.NewEncoder(w).Encode(stats)
        }
    }
    ```

    4. Create `backend/pkg/database/stats_test.go`:

    ```go
    package database_test

    import (
        "encoding/json"
        "net/http"
        "net/http/httptest"
        "testing"

        "github.com/CodeWarrior-debug/perspectize/backend/pkg/database"
        "github.com/stretchr/testify/assert"
        "github.com/stretchr/testify/require"

        _ "github.com/jackc/pgx/v5/stdlib"
        sqlpkg "database/sql"
    )

    func TestStatsHandler(t *testing.T) {
        // Open a no-op sql.DB to get valid stats (doesn't need a real connection)
        db, err := sqlpkg.Open("pgx", "postgres://invalid:5432/fake?sslmode=disable")
        require.NoError(t, err)
        defer db.Close()

        handler := database.StatsHandler(db)
        req := httptest.NewRequest(http.MethodGet, "/debug/db-stats", nil)
        rec := httptest.NewRecorder()

        handler.ServeHTTP(rec, req)

        assert.Equal(t, http.StatusOK, rec.Code)
        assert.Equal(t, "application/json", rec.Header().Get("Content-Type"))

        var stats sqlpkg.DBStats
        err = json.NewDecoder(rec.Body).Decode(&stats)
        assert.NoError(t, err)
    }
    ```

    5. Run `cd backend && go test ./pkg/database/... -v` — must pass (slow query test may skip without DB).
  </action>
  <verify>
    `cd backend && go test ./pkg/database/... -v` passes. `grep "RegisterSlowQueryLogger" backend/pkg/database/plugins.go` returns match. `grep "StatsHandler" backend/pkg/database/stats.go` returns match.
  </verify>
  <done>
    GORM slow query logger registers callbacks for query/create/update/delete operations, logging warnings for queries exceeding 100ms. StatsHandler serves sql.DBStats as JSON. Tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 3: GraphQL operation timing extension</name>
  <files>
    backend/pkg/graphql/timing.go
    backend/pkg/graphql/timing_test.go
  </files>
  <action>
    1. Create `backend/pkg/graphql/timing.go`:

    ```go
    package graphql

    import (
        "context"
        "log/slog"
        "time"

        "github.com/99designs/gqlgen/graphql"
    )

    // OperationTimer returns a gqlgen AroundOperations middleware that logs
    // the operation name and duration for every GraphQL request.
    func OperationTimer() graphql.OperationInterceptor {
        return graphql.OperationInterceptorFunc(func(ctx context.Context, next graphql.OperationHandler) graphql.ResponseHandler {
            start := time.Now()
            oc := graphql.GetOperationContext(ctx)

            rh := next(ctx)

            operationName := "anonymous"
            if oc != nil && oc.OperationName != "" {
                operationName = oc.OperationName
            }

            slog.InfoContext(ctx, "graphql",
                "operation", operationName,
                "duration_ms", time.Since(start).Milliseconds(),
            )

            return rh
        })
    }
    ```

    2. Create `backend/pkg/graphql/timing_test.go`:

    ```go
    package graphql_test

    import (
        "testing"

        gqltiming "github.com/CodeWarrior-debug/perspectize/backend/pkg/graphql"
        "github.com/stretchr/testify/assert"
    )

    func TestOperationTimer_ReturnsInterceptor(t *testing.T) {
        interceptor := gqltiming.OperationTimer()
        assert.NotNil(t, interceptor)
    }
    ```

    3. Run `cd backend && go test ./pkg/graphql/... -v` — must pass.
  </action>
  <verify>
    `cd backend && go test ./pkg/graphql/... -v` passes. `grep "OperationTimer" backend/pkg/graphql/timing.go` returns match.
  </verify>
  <done>
    gqlgen AroundOperations extension logs operation name and duration_ms for every GraphQL request. Test passes.
  </done>
</task>

<task type="auto">
  <name>Task 4: Wire all backend monitoring into main.go</name>
  <files>
    backend/cmd/server/main.go
  </files>
  <action>
    1. Add imports to `main.go`:
       ```go
       perfmw "github.com/CodeWarrior-debug/perspectize/backend/pkg/middleware"
       gqltiming "github.com/CodeWarrior-debug/perspectize/backend/pkg/graphql"
       ```

    2. After `database.ConnectGORM(dsn, poolCfg)` succeeds, register the slow query logger:
       ```go
       database.RegisterSlowQueryLogger(db)
       ```

    3. In the middleware chain, add `perfmw.RequestTimer` AFTER `middleware.RequestID` and `middleware.RealIP`, but BEFORE `middleware.Logger`:
       ```go
       r.Use(middleware.RequestID)
       r.Use(middleware.RealIP)
       r.Use(perfmw.RequestTimer)
       r.Use(middleware.Recoverer)
       ```
       Remove `middleware.Logger` since `RequestTimer` replaces its functionality with richer structured fields.

    4. After creating the gqlgen server, add the operation timer:
       ```go
       srv := handler.NewDefaultServer(generated.NewExecutableSchema(generated.Config{Resolvers: resolver}))
       srv.AroundOperations(gqltiming.OperationTimer())
       ```

    5. Add the DB stats endpoint (dev-only, next to /health and /ready):
       ```go
       if os.Getenv("APP_ENV") != "production" {
           r.Get("/debug/db-stats", database.StatsHandler(sqlDB))
       }
       ```

    6. Run `cd backend && go build ./...` — must compile.
  </action>
  <verify>
    `cd backend && go build ./...` compiles. `grep "RequestTimer" backend/cmd/server/main.go` returns match. `grep "RegisterSlowQueryLogger" backend/cmd/server/main.go` returns match. `grep "OperationTimer" backend/cmd/server/main.go` returns match. `grep "db-stats" backend/cmd/server/main.go` returns match.
  </verify>
  <done>
    All backend monitoring wired: request timing middleware, GORM slow query logger, GraphQL operation timer, and /debug/db-stats endpoint. chi's Logger middleware replaced by richer RequestTimer.
  </done>
</task>

<task type="auto">
  <name>Task 5: Go benchmark tests for service layer</name>
  <files>
    backend/internal/core/services/content_service_bench_test.go
    backend/internal/core/services/perspective_service_bench_test.go
  </files>
  <action>
    1. Review existing mock implementations in the test files to understand the mock patterns used.

    2. Create `backend/internal/core/services/content_service_bench_test.go`:
       - Benchmark `ContentService.GetByID` with a mocked repository
       - Benchmark `ContentService.ListContent` with mocked repository returning 10 items
       - Use the same mock pattern as existing unit tests

    3. Create `backend/internal/core/services/perspective_service_bench_test.go`:
       - Benchmark `PerspectiveService.GetByID` with a mocked repository
       - Benchmark `PerspectiveService.ListPerspectives` with mocked repository returning 10 items

    4. Run `cd backend && go test -bench=. -benchmem ./internal/core/services/...` — must produce benchmark output.

    5. Verify existing tests still pass: `cd backend && go test ./...`
  </action>
  <verify>
    `cd backend && go test -bench=. -benchmem ./internal/core/services/... -benchtime=1s` produces benchmark output with ns/op and B/op. `cd backend && go test ./...` all tests pass.
  </verify>
  <done>
    Benchmark tests established for ContentService and PerspectiveService. Reproducible baselines can be tracked via `go test -bench`.
  </done>
</task>

<task type="auto">
  <name>Task 6: Frontend Web Vitals baseline</name>
  <files>
    frontend/src/lib/vitals.ts
    frontend/src/routes/+layout.svelte
  </files>
  <action>
    1. Install web-vitals: `cd frontend && pnpm add web-vitals`

    2. Create `frontend/src/lib/vitals.ts`:

    ```typescript
    import type { Metric } from 'web-vitals';

    function sendToAnalytics(metric: Metric) {
        // In development, log to console. In production, this could POST to an endpoint.
        console.debug('[Web Vitals]', metric.name, Math.round(metric.value), 'ms', {
            id: metric.id,
            rating: metric.rating,
        });
    }

    export function reportWebVitals() {
        import('web-vitals').then(({ onCLS, onINP, onLCP, onFCP, onTTFB }) => {
            onCLS(sendToAnalytics);
            onINP(sendToAnalytics);
            onLCP(sendToAnalytics);
            onFCP(sendToAnalytics);
            onTTFB(sendToAnalytics);
        });
    }
    ```

    3. Update `frontend/src/routes/+layout.svelte` to call `reportWebVitals` on mount:

    Add to the script block:
    ```typescript
    import { onMount } from 'svelte';
    import { reportWebVitals } from '$lib/vitals';

    onMount(() => {
        reportWebVitals();
    });
    ```

    4. Run `cd frontend && pnpm run test:run` — all tests must pass.
  </action>
  <verify>
    `cd frontend && pnpm run test:run` passes. `grep "reportWebVitals" frontend/src/lib/vitals.ts` returns match. `grep "reportWebVitals" frontend/src/routes/+layout.svelte` returns match.
  </verify>
  <done>
    Frontend captures all Core Web Vitals (LCP, CLS, INP, FCP, TTFB) and logs to console in dev. Ready for production analytics endpoint when needed.
  </done>
</task>

</tasks>

<verification>
1. `cd backend && go build ./...` — zero errors
2. `cd backend && go test ./...` — all tests pass
3. `cd backend && go test -bench=. -benchmem ./internal/core/services/...` — benchmarks produce output
4. `cd frontend && pnpm run test:run` — all tests pass
5. Request timing: `grep "RequestTimer" backend/cmd/server/main.go`
6. Slow query logger: `grep "RegisterSlowQueryLogger" backend/cmd/server/main.go`
7. GraphQL timing: `grep "OperationTimer" backend/cmd/server/main.go`
8. DB stats endpoint: `grep "db-stats" backend/cmd/server/main.go`
9. Web Vitals: `grep "reportWebVitals" frontend/src/routes/+layout.svelte`
</verification>

<success_criteria>
- Every HTTP request logs method, path, status, duration_ms, bytes, request_id via slog
- GORM slow queries (>100ms) are logged with operation, duration, SQL, and row count
- /debug/db-stats returns sql.DBStats as JSON (dev-only)
- GraphQL operations log operation name and duration_ms
- Benchmark tests exist for ContentService and PerspectiveService
- Frontend captures LCP, CLS, INP, FCP, TTFB and logs to console
- All existing tests pass — no regressions
</success_criteria>

<output>
After completion, create `.planning/phases/07.4-performance-monitoring/07.4-01-SUMMARY.md`
</output>
